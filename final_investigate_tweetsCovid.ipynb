{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOtbZZC9BsBdizUslsBDKq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ditumystro/Insurance_Data_Analysis/blob/main/final_investigate_tweetsCovid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWujNmoAvPBR"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers \n",
        "!pip install bertopic[all] "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import numpy as np\n",
        "import csv\n",
        "import urllib.request\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from __future__ import print_function\n",
        "import nltk\n",
        "import os\n",
        "import codecs\n",
        "from sklearn import feature_extraction\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import joblib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "from sklearn.manifold import MDS\n",
        "from scipy.cluster.hierarchy import ward, dendrogram\n",
        "#from spellchecker import SpellChecker\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import string\n",
        "\n",
        "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
        "from transformers import TokenClassificationPipeline\n",
        "from datetime import date, timedelta \n",
        "\n",
        "from datetime import *\n",
        "from dateutil import parser\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "random.seed(50)\n",
        "keywords = ['mask','face covering', 'face cover', 'facecover']"
      ],
      "metadata": {
        "id": "fXVgSS0-vRyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "def text_preprocessing(text):\n",
        "  text = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in text]\n",
        "  text = [re.sub('(@.*?)[\\s]', '', sent) for sent in text]\n",
        "  text = [re.sub('\\s+', ' ', sent) for sent in text]\n",
        "  text = [re.sub('http[s]?://\\S+', '', sent) for sent in text]\n",
        "# text = [re.sub(r'[0-9]+', '', sent) for sent in text]\n",
        "  return text "
      ],
      "metadata": {
        "id": "7E4Kw5O9vcoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://www.dropbox.com/s/oal4ia7oae1d5dh/stage_two_df.csv\n",
        "stage_two_df = pd.read_csv(\"stage_two_df.csv\", sep=',')\n",
        "!rm stage_two_df.csv\n",
        "\n",
        "\n",
        "###Consider only retweeted tweets to shrink the amount of data\n",
        "\n",
        "###Duplicate this script  (1) run stage data with only retweeted tweets (2) run bi-monthly datasets"
      ],
      "metadata": {
        "id": "RBzRvQNpvv7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage_two_df.shape"
      ],
      "metadata": {
        "id": "Ho31L5egvx2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stage_two_df = stage_two_df[stage_two_df['col37'] > 0] \n",
        "stage_two_df.shape "
      ],
      "metadata": {
        "id": "hygp9mOav6av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hashtags(text):\n",
        "  regex = \"#(\\w+)\"\n",
        "  hashtag_list = re.findall(regex, text)\n",
        "  if len(hashtag_list) > 0: hashtag_list = hashtag_list\n",
        "  return hashtag_list "
      ],
      "metadata": {
        "id": "uMJstNqlwApj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bio_description = stage_two_df.col21.to_list()\n",
        "bio_description = text_preprocessing(bio_description)\n",
        "\n",
        "tweets = stage_two_df.col4.to_list() \n",
        "\n",
        "bio_users = stage_two_df.col19.to_list()"
      ],
      "metadata": {
        "id": "FVkU0j8OwCAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extracting hashtags from bio description\n",
        "hashtag_df = pd.DataFrame()\n",
        "\n",
        "for i in range(len(bio_description)):\n",
        "  bio = bio_description[i]\n",
        "  bio = extract_hashtags(bio)\n",
        "  \n",
        "  data = pd.DataFrame(bio)\n",
        "  hashtag_df = hashtag_df.append(data)    "
      ],
      "metadata": {
        "id": "Gh3IsGCjwJyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashtag_df.columns = ['hashtag']\n",
        "hashtag_df['hashtag'] = hashtag_df['hashtag'].str.lower()"
      ],
      "metadata": {
        "id": "d1h3ZUZJwLKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOC = hashtag_df.groupby(['hashtag']).size().to_frame('count').reset_index()\n",
        "LOC = LOC.sort_values(\"count\", ascending=False)\n",
        "\n",
        "LOC.shape"
      ],
      "metadata": {
        "id": "hZ8hPdF3wR2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOC_hd = LOC.head(200)"
      ],
      "metadata": {
        "id": "V2nfpVCUwRwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.catplot(x=\"count\", y=\"hashtag\", kind=\"bar\", height=28.27, aspect=11.7/28.27, data=LOC_hd)"
      ],
      "metadata": {
        "id": "TsQFjnlEwYLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "gZCdDKzcwZ-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "7LTpO4AYw0IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def tokenize_and_stem(text):\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    tokens = [word for sent in nltk.sent_tokenize(\n",
        "        text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "\n",
        "    return stems\n",
        "\n",
        "def tokenize_only(text):\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text)\n",
        "              for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    return filtered_tokens\n",
        "\n",
        "\n",
        "totalvocab_stemmed = []\n",
        "totalvocab_tokenized = [] "
      ],
      "metadata": {
        "id": "1AhT-BkAw2c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in bio_description:\n",
        "  replace_punctuation = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "  i = i.translate(replace_punctuation)\n",
        "  allwords_stemmed = tokenize_and_stem(i)\n",
        "  totalvocab_stemmed.extend(allwords_stemmed)\n",
        "  allwords_tokenized = tokenize_only(i)\n",
        "  totalvocab_tokenized.extend(allwords_tokenized) "
      ],
      "metadata": {
        "id": "VBNYNDrUw2Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index=totalvocab_stemmed)"
      ],
      "metadata": {
        "id": "PHchZRFWw2JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english',tokenizer=tokenize_and_stem)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(bio_description)\n",
        "\n",
        "print(tfidf_matrix.shape)\n",
        "terms = tfidf_vectorizer.get_feature_names() "
      ],
      "metadata": {
        "id": "PszaeVo6xHzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "#Vary number of clusters: 5, 10, 15, 20\n",
        "nclusters = [5,10,15,20]\n",
        "for clst in range(len(nclusters)):\n",
        "  nclst = nclusters[clst]\n",
        "\n",
        "  print(\"Num clusters: %d\" % nclst, end='')\n",
        "  print()\n",
        "  print()\n",
        "\n",
        "  km = KMeans(n_clusters=nclst)\n",
        "  km.fit(tfidf_matrix)\n",
        "  clusters = km.labels_.tolist()\n",
        "  \n",
        "  order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "  for i in range(nclst):\n",
        "    print(\"Cluster %d words:\" % i, end='')\n",
        "    for ind in order_centroids[i, :20]: \n",
        "      print(' %s' % terms[ind], end='')\n",
        "      print()\n",
        "    print()"
      ],
      "metadata": {
        "id": "eiG6hl96xMKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Number clusters = 5\n",
        "num_clusters = 5\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "km.fit(tfidf_matrix)\n",
        "\n",
        "clusters = km.labels_.tolist()\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "for i in range(num_clusters):\n",
        "  print(\"Cluster %d words:\" % i, end='')\n",
        "  \n",
        "  for ind in order_centroids[i, :20]: \n",
        "    print(' %s' % terms[ind], end='')\n",
        "  print()"
      ],
      "metadata": {
        "id": "mvtBU6auxQNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Number of users by cluster\n",
        "df = {'bio': bio_description, 'cluster': clusters, 'user': bio_users, 'tweets': tweets}\n",
        "frame = pd.DataFrame(df, columns=['bio','cluster', 'user', 'tweets'])\n",
        "\n",
        "cluster_df = frame.groupby(['cluster']).size().to_frame('count').reset_index()\n",
        "cluster_df = cluster_df.sort_values(\"count\", ascending=False)\n",
        "sns.catplot(x=\"cluster\", y=\"count\", kind=\"bar\", data=cluster_df) "
      ],
      "metadata": {
        "id": "YGdPpuLDxTnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Number clusters = 10\n",
        "num_clusters = 10\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "km.fit(tfidf_matrix)\n",
        "\n",
        "clusters = km.labels_.tolist()\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "for i in range(num_clusters):\n",
        "  print(\"Cluster %d words:\" % i, end='')\n",
        "  \n",
        "  for ind in order_centroids[i, :20]: \n",
        "    print(' %s' % terms[ind], end='')\n",
        "  print()"
      ],
      "metadata": {
        "id": "ETD11KdQxYJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Number of users by cluster\n",
        "df = {'bio': bio_description, 'cluster': clusters, 'user': bio_users, 'tweets': tweets}\n",
        "frame = pd.DataFrame(df, columns=['bio','cluster', 'user', 'tweets'])\n",
        "\n",
        "cluster_df = frame.groupby(['cluster']).size().to_frame('count').reset_index()\n",
        "cluster_df = cluster_df.sort_values(\"count\", ascending=False)\n",
        "sns.catplot(x=\"cluster\", y=\"count\", kind=\"bar\", data=cluster_df) "
      ],
      "metadata": {
        "id": "EahRiDtSxceo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Number clusters = 15\n",
        "num_clusters = 15\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "km.fit(tfidf_matrix)\n",
        "\n",
        "clusters = km.labels_.tolist()\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "for i in range(num_clusters):\n",
        "  print(\"Cluster %d words:\" % i, end='')\n",
        "  \n",
        "  for ind in order_centroids[i, :20]: \n",
        "    print(' %s' % terms[ind], end='')\n",
        "  print() "
      ],
      "metadata": {
        "id": "CqzgcBV3xgtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Number of users by cluster\n",
        "df = {'bio': bio_description, 'cluster': clusters, 'user': bio_users, 'tweets': tweets}\n",
        "frame = pd.DataFrame(df, columns=['bio','cluster', 'user', 'tweets'])\n",
        "\n",
        "cluster_df = frame.groupby(['cluster']).size().to_frame('count').reset_index()\n",
        "cluster_df = cluster_df.sort_values(\"count\", ascending=False)\n",
        "sns.catplot(x=\"cluster\", y=\"count\", kind=\"bar\", data=cluster_df) "
      ],
      "metadata": {
        "id": "u59SzmyLxlE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Number clusters = 20\n",
        "num_clusters = 20\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "km.fit(tfidf_matrix)\n",
        "\n",
        "clusters = km.labels_.tolist()\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "for i in range(num_clusters):\n",
        "  print(\"Cluster %d words:\" % i, end='')\n",
        "  \n",
        "  for ind in order_centroids[i, :20]: \n",
        "    print(' %s' % terms[ind], end='')\n",
        "  print() "
      ],
      "metadata": {
        "id": "pdUNW9f5xpLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Number of users by cluster\n",
        "df = {'bio': bio_description, 'cluster': clusters, 'user': bio_users, 'tweets': tweets}\n",
        "frame = pd.DataFrame(df, columns=['bio','cluster', 'user', 'tweets'])\n",
        "\n",
        "cluster_df = frame.groupby(['cluster']).size().to_frame('count').reset_index()\n",
        "cluster_df = cluster_df.sort_values(\"count\", ascending=False)\n",
        "sns.catplot(x=\"cluster\", y=\"count\", kind=\"bar\", data=cluster_df) "
      ],
      "metadata": {
        "id": "R85JZ1rlxx8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "% matplotlib inline\n",
        "\n",
        "x = []\n",
        "y = []\n",
        "yerr = []\n",
        "nclusters = [5,10,15,20]\n",
        "\n",
        "for i in range(len(nclusters)):\n",
        "    n_clusters = nclusters[i]\n",
        "    km = KMeans(n_clusters=n_clusters)\n",
        "    \n",
        "    sscore = []\n",
        "    for i in range(1, 2):\n",
        "        km.fit(tfidf_matrix)\n",
        "        sscore.append(metrics.silhouette_score(tfidf_matrix, km.labels_))\n",
        "    \n",
        "    x.append(n_clusters)\n",
        "    y.append(np.median(sscore))\n",
        "    yerr.append(np.std(sscore))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette score value')\n",
        "plt.errorbar(x, y, yerr=yerr, fmt='-o', ecolor='g', color='g')\n",
        "plt.xlim(xmin=5, xmax=20)\n",
        "plt.ylim(ymin=0, ymax=0.015)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QlE6S0LXx2c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Applying BERTopic on Tweets stemming from users in Cluster 18\n",
        "cluster_df_sub = frame[frame['cluster'].isin(['1'])]\n",
        "clt_bio_users = cluster_df_sub.user.to_list()\n",
        "\n",
        "tsv_read_clst = stage_two_df[stage_two_df['col19'].isin(clt_bio_users)]\n"
      ],
      "metadata": {
        "id": "eAchP1bhx2aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Loading the NER pipeline by using a BERT based model and tokenizer.\n",
        "bert_model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") \n",
        "ner_model = TokenClassificationPipeline( model = bert_model, tokenizer = tokenizer, grouped_entities=True)"
      ],
      "metadata": {
        "id": "upXExUyQx2Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manage_timestamps(text):\n",
        "  text = parser.parse(text)\n",
        "  return text.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "def manage_dt(text):\n",
        "  text = parser.parse(text)\n",
        "  return text.strftime('%Y-%m-%d')\n",
        "\n",
        "def manage_dates(text):\n",
        "  return text.date() "
      ],
      "metadata": {
        "id": "OLfHJaBcx2UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "tsv_read_clst['timestamp'] = tsv_read_clst.col1.apply(manage_timestamps)\n",
        "tsv_read_clst.col4 = tsv_read_clst.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.col4).lower(), 1)\n",
        "tsv_read_clst.col4 = tsv_read_clst.apply(lambda row: \" \".join(filter(lambda x:x[0]!=\"@\", row.col4.split())), 1)\n",
        "tsv_read_clst.col4 = tsv_read_clst.apply(lambda row: \" \".join(re.sub(\"[^a-zA-Z]+\", \" \", row.col4).split()), 1)\n",
        "\n",
        "timestamps = tsv_read_clst.timestamp.to_list()\n",
        "tweets = tsv_read_clst.col4.to_list() "
      ],
      "metadata": {
        "id": "-8Ecrma9x2Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsv_read_clst.shape "
      ],
      "metadata": {
        "id": "bdPG719px2Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
        "model = BERTopic(verbose=True, n_gram_range=(1, 1), calculate_probabilities=True, min_topic_size=15, vectorizer_model=vec)\n",
        "topics, _ = model.fit_transform(tweets) "
      ],
      "metadata": {
        "id": "gCdyFEp8yZxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics_over_time = model.topics_over_time(tweets, topics, timestamps) "
      ],
      "metadata": {
        "id": "fQugRqh2ye58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualizing topics\n",
        "model.visualize_topics() "
      ],
      "metadata": {
        "id": "P2f_eIoxyifi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics_ot = topics_over_time\n",
        "topics_ot['Date'] = topics_ot.Timestamp.apply(manage_dates)\n",
        "\n",
        "topics_ot "
      ],
      "metadata": {
        "id": "sXKkt5GRymlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "over_time = topics_ot.groupby(['Topic', 'Date'])['Frequency'].sum().reset_index()\n",
        "topics_ot = topics_ot.groupby(['Topic'])['Frequency'].sum().reset_index()\n",
        "topics_ot.sort_values(\"Frequency\", ascending = False).head(10) "
      ],
      "metadata": {
        "id": "YU1RvpCZyqIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Top 10 topics.\n",
        "topics_ot = topics_ot.sort_values(\"Frequency\", ascending = False)\n",
        "\n",
        "topics_ot['C'] = topics_ot.reset_index().index\n",
        "topics_ftd = topics_ot.loc[~(topics_ot['Topic'] == -1)]\n",
        "\n",
        "\n",
        "topics_filtered = topics_ftd[topics_ftd['C'].isin([0,1,2,3,4,5,6,7,8,9,10])]\n",
        "topics_filtered = topics_filtered.Topic.to_list()\n",
        "otf = over_time.Topic.isin(topics_filtered)\n",
        "over_time_filtered = over_time[otf]\n",
        "\n",
        "sns.set(rc={'figure.figsize':(18.7,8.27)})\n",
        "\n",
        "\n",
        "sns.lineplot(x=\"Date\",\n",
        "             y=\"Frequency\", \n",
        "             data = over_time_filtered, \n",
        "             hue=\"Topic\", \n",
        "             palette=\"Dark2\") "
      ],
      "metadata": {
        "id": "pBFqSG8VyvYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O41SkYjOyvVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V9UAM60IyvR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N2nAlGd6yvPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SjVCGgVtyvL4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}